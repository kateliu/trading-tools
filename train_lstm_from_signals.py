#!/usr/bin/env python3
"""Train an LSTM on engineered signal features to forecast daily closing prices."""

from __future__ import annotations

import argparse
import json
import math
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Sequence

import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset


@dataclass
class Scaler:
    mean: np.ndarray
    std: np.ndarray

    def transform(self, values: np.ndarray) -> np.ndarray:
        return (values - self.mean) / self.std

    def inverse_transform(self, values: np.ndarray, index: int) -> np.ndarray:
        return values * self.std[index] + self.mean[index]


class SequenceDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.as_tensor(X, dtype=torch.float32)
        self.y = torch.as_tensor(y, dtype=torch.float32)

    def __len__(self) -> int:  # pragma: no cover - trivial
        return self.X.shape[0]

    def __getitem__(self, idx: int):  # pragma: no cover - trivial
        return self.X[idx], self.y[idx]


class LSTMRegressor(nn.Module):
    def __init__(self, input_size: int, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0.0,
            batch_first=True,
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        output, _ = self.lstm(x)
        output = output[:, -1, :]
        return self.fc(output).squeeze(-1)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train an LSTM on signal features to forecast stock price.")
    parser.add_argument("signals_csv", type=Path, help="CSV generated by fetch_signals.py (indexed by date).")
    parser.add_argument("--lookback", type=int, default=60, help="Number of past days per sequence (default: 60)")
    parser.add_argument("--epochs", type=int, default=50, help="Training epochs (default: 50)")
    parser.add_argument("--batch-size", type=int, default=64, help="Batch size (default: 64)")
    parser.add_argument("--learning-rate", type=float, default=1e-3, help="Learning rate (default: 1e-3)")
    parser.add_argument("--forecast-days", type=int, default=30, help="Forecast horizon to generate (default: 30)")
    parser.add_argument("--model-output", type=Path, default=Path("models/signals_lstm.pt"), help="Path for model weights")
    parser.add_argument("--metadata-output", type=Path, default=Path("models/signals_lstm.json"), help="Path for metadata")
    parser.add_argument("--forecast-json", type=Path, default=Path("web/data/ai_forecast.json"), help="Forecast JSON output")
    parser.add_argument("--device", default="cpu", help="Torch device spec (default: cpu)")
    return parser.parse_args()


def load_signals(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path, parse_dates=["date"], index_col="date")
    required = {"close"}
    missing = required - set(df.columns)
    if missing:
        raise SystemExit(f"Signals CSV missing required column(s): {sorted(missing)}")
    df.sort_index(inplace=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)
    if df.empty:
        raise SystemExit("Signals CSV contains no usable data after cleaning.")
    return df


def build_sequences(df: pd.DataFrame, lookback: int) -> tuple[np.ndarray, np.ndarray, Scaler, List[str]]:
    feature_cols = [col for col in df.columns if col != "ticker"]
    if "ticker" in feature_cols:
        feature_cols.remove("ticker")

    feature_array = df[feature_cols].to_numpy(dtype=np.float64)
    if feature_array.shape[0] <= lookback:
        raise SystemExit("Not enough rows for the requested lookback window.")

    mean = feature_array.mean(axis=0)
    std = feature_array.std(axis=0)
    std[std == 0] = 1.0
    scaler = Scaler(mean=mean, std=std)
    scaled = scaler.transform(feature_array)

    close_index = feature_cols.index("close")

    X, y = [], []
    for start in range(0, len(scaled) - lookback):
        end = start + lookback
        target_idx = end
        if target_idx >= len(scaled):
            break
        X.append(scaled[start:end])
        y.append(scaled[target_idx, close_index])

    return np.stack(X), np.array(y), scaler, feature_cols


def split_datasets(features: np.ndarray, targets: np.ndarray, train_ratio: float = 0.8):
    split = int(len(features) * train_ratio)
    split = min(max(split, 1), len(features) - 1)
    return (
        SequenceDataset(features[:split], targets[:split]),
        SequenceDataset(features[split:], targets[split:]),
    )


def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int, optimizer, device):
    criterion = nn.MSELoss()
    for epoch in range(1, epochs + 1):
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * X_batch.size(0)
        train_loss /= len(train_loader.dataset)

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)
                preds = model(X_batch)
                loss = criterion(preds, y_batch)
                val_loss += loss.item() * X_batch.size(0)
        val_loss /= len(val_loader.dataset)

        print(f"[train] Epoch {epoch:03d} | train_loss={train_loss:.6f} val_loss={val_loss:.6f}")


def evaluate(model: nn.Module, loader: DataLoader, device: torch.device, scaler: Scaler, close_index: int) -> Dict[str, float]:
    preds, actuals = [], []
    criterion = nn.MSELoss()
    mse_accum = 0.0
    model.eval()
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            outputs = model(X_batch)
            mse_accum += criterion(outputs, y_batch).item() * X_batch.size(0)
            preds.extend(outputs.cpu().numpy())
            actuals.extend(y_batch.cpu().numpy())

    preds = np.array(preds)
    actuals = np.array(actuals)
    preds_real = scaler.inverse_transform(preds, close_index)
    actuals_real = scaler.inverse_transform(actuals, close_index)

    mse = np.mean((preds_real - actuals_real) ** 2)
    rmse = math.sqrt(mse)
    mae = np.mean(np.abs(preds_real - actuals_real))
    denom = np.where(np.abs(actuals_real) < 1e-6, np.nan, actuals_real)
    mape = float(np.nan_to_num(np.mean(np.abs((preds_real - actuals_real) / denom) * 100), nan=0.0))

    return {"rmse": rmse, "mae": mae, "mape": mape}


def iterative_forecast(
    model: nn.Module,
    df: pd.DataFrame,
    scaler: Scaler,
    feature_cols: List[str],
    lookback: int,
    forecast_days: int,
    close_index: int,
    device: torch.device,
) -> List[Dict[str, float]]:
    if forecast_days <= 0:
        return []

    array = df[feature_cols].to_numpy(dtype=np.float64)
    window = scaler.transform(array)[-lookback:].copy()
    last_date = df.index[-1]
    predictions: List[Dict[str, float]] = []

    for _ in range(forecast_days):
        tensor = torch.as_tensor(window[np.newaxis, :, :], dtype=torch.float32, device=device)
        with torch.no_grad():
            pred_scaled = model(tensor).cpu().item()
        pred_close = float(scaler.inverse_transform(np.array([pred_scaled]), close_index)[0])

        next_date = next_market_day(last_date)
        predictions.append({"date": next_date.strftime("%Y-%m-%d"), "close": round(pred_close, 4)})
        last_date = next_date

        new_row = window[-1].copy()
        new_row[close_index] = pred_scaled
        window = np.vstack([window[1:], new_row])

    return predictions


def next_market_day(date: pd.Timestamp) -> pd.Timestamp:
    candidate = date + pd.Timedelta(days=1)
    while candidate.weekday() >= 5:
        candidate += pd.Timedelta(days=1)
    return candidate


def save_artifacts(model, model_path: Path, metadata_path: Path, scaler: Scaler, feature_cols: List[str], metrics: Dict[str, float], args: argparse.Namespace):
    model_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save({"model_state": model.state_dict(), "feature_cols": feature_cols}, model_path)

    metadata = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "lookback": args.lookback,
        "feature_mean": scaler.mean.tolist(),
        "feature_std": scaler.std.tolist(),
        "metrics": metrics,
    }
    metadata_path.parent.mkdir(parents=True, exist_ok=True)
    metadata_path.write_text(json.dumps(metadata, indent=2))


def save_forecast(path: Path, predictions: Sequence[Dict[str, float]], metrics: Dict[str, float], args: argparse.Namespace) -> None:
    payload = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "lookback": args.lookback,
        "forecast_days": args.forecast_days,
        "metrics": metrics,
        "predictions": list(predictions),
        "source": "lstm-signals",
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2))
    print(f"[train] Forecast written to {path}")


def main() -> int:
    args = parse_args()
    device = torch.device(args.device)

    signals_df = load_signals(args.signals_csv)
    features, targets, scaler, feature_cols = build_sequences(signals_df, args.lookback)
    train_ds, val_ds = split_datasets(features, targets)

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=args.batch_size)

    model = LSTMRegressor(input_size=len(feature_cols)).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    train_model(model, train_loader, val_loader, args.epochs, optimizer, device)
    metrics = evaluate(model, val_loader, device, scaler, feature_cols.index("close"))
    print(f"[train] Evaluation metrics: {json.dumps(metrics, indent=2)}")

    predictions = iterative_forecast(
        model,
        signals_df,
        scaler,
        feature_cols,
        args.lookback,
        args.forecast_days,
        feature_cols.index("close"),
        device,
    )

    save_artifacts(model, args.model_output, args.metadata_output, scaler, feature_cols, metrics, args)
    save_forecast(args.forecast_json, predictions, metrics, args)

    print("[train] Training complete.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
